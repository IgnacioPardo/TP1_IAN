{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Crear una instancia del ambiente\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agente import AgenteQLearning\n",
    "#from DQN import AgenteDQN\n",
    "from ambiente import AmbienteDiezMil\n",
    "from template import Validador\n",
    "#from jugador import JugadorFromDQNPolicy\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from utils import RangeSnaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del gráfico\n",
    "def train(epochs, episodios, alpha, gamma, epsilon, rs, early_stop = 20, seed = 42):    \n",
    "    seed = seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ambiente = AmbienteDiezMil(rs=rs)\n",
    "\n",
    "    # Crear un agente de Q-learning\n",
    "    agente = AgenteQLearning(ambiente, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "\n",
    "    vals_normal = []\n",
    "    best_validation = 30\n",
    "    best_policy = None\n",
    "    epochs_sin_mejora = 0\n",
    "    # Entrenar al agente con un número de episodios\n",
    "    for _ in range(epochs):\n",
    "        agente.entrenar(episodios, verbose=False)\n",
    "        if _ % 2 == 0:\n",
    "            validador = Validador(AmbienteDiezMil(rs=rs))\n",
    "            vals_normal.append(validador.validar_politica(agente.q_table2pol(), 300))\n",
    "            if vals_normal[-1] < best_validation:\n",
    "                best_validation = vals_normal[-1]\n",
    "                best_policy = agente.q_table2pol()\n",
    "                epochs_sin_mejora = 0\n",
    "            else:\n",
    "                epochs_sin_mejora += 1\n",
    "                if epochs_sin_mejora == early_stop:\n",
    "                    break\n",
    "    \n",
    "    return best_validation, best_policy, vals_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv, bp, vn = train(100, 100, 0.1, 0.9, 0.1, RangeSnaper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.32,\n",
       " array([[1, 0, 0, 1, 1, 1, 1],\n",
       "        [1, 1, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 1, 0, 1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bv, bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGOS1 = [\n",
    "    (0, 1), (1, 250), (250, 500),(500, 750), (750, 1000), (1000, 2000), (2000, 5000), (5000, 10000), (10000, int(1e10))\n",
    "]\n",
    "\n",
    "RANGOS2 = [\n",
    "    (0, 1),(1, 50),(50, 100), (100, 150), (150, 200), (200, 250),\n",
    "    (250, 350), (350, 500), (500, 750), (750, 1000), (1000, 2000),\n",
    "    (2000, 5000), (5000, 10000), (10000, int(1e10))\n",
    "]\n",
    "\n",
    "RANGOS3 = [(0, 1), (1, 50), (50, 100), (100, 150), (150, 200), \n",
    "           (200, 250), (250, 350), (350, 500), (500, 750), (750, 1000), \n",
    "           (1000, 2000), (2000, 4000), (4000, 6000), (6000,8000),(8000,10000),(10000, int(1e10))]\n",
    "\n",
    "RANGOS4 = [\n",
    "    (0,1), (1, 250), (250, 500),(500, 750), (750, 1000),(1000, 5000), (5000, int(1e10))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "#hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def objective(params):\n",
    "    alpha = params[\"alpha\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    #reward = params[\"reward\"]\n",
    "    rango = params[\"rango\"]\n",
    "\n",
    "    best_validation, best_policy, _ = train(epochs=500, episodios=1000, alpha=alpha, gamma=gamma, epsilon=epsilon, rs=RangeSnaper(rango))\n",
    "    \n",
    "    return {\n",
    "        \"loss\": best_validation,\n",
    "        \"status\": STATUS_OK,\n",
    "        \"best_policy\": best_policy\n",
    "    }\n",
    "\n",
    "space = {\n",
    "    \"alpha\": hp.uniform(\"alpha\", 0.0, 1.0),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0.0, 1.0),\n",
    "    \"epsilon\": hp.uniform(\"epsilon\", 0.01, 0.2),\n",
    "    #\"reward\": hp.choice(\"reward\", [0, 1, 2, 3]),\n",
    "    \"rango\": hp.choice(\"rango\", [RANGOS1, RANGOS2, RANGOS4])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials, verbose=1)\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestModelfromTrials(trials):\n",
    "    valid_trial_list = [trial for trial in trials\n",
    "                            if STATUS_OK == trial['result']['status']]\n",
    "    losses = [ float(trial['result']['loss']) for trial in valid_trial_list]\n",
    "    index_having_minumum_loss = np.argmin(losses)\n",
    "    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n",
    "    return best_trial_obj['result']['best_policy']\n",
    "\n",
    "best_policy = getBestModelfromTrials(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.073333333333334"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validador = Validador(AmbienteDiezMil(rs=RangeSnaper()))\n",
    "(validador.validar_politica(best_policy, 300))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.67, 21.14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bv, bp , _ =train(100, 1000, best[\"alpha\"], best[\"gamma\"], best[\"epsilon\"], RangeSnaper())\n",
    "validador = Validador(AmbienteDiezMil(rs=RangeSnaper()))\n",
    "(validador.validar_politica(bp, 300)), bv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros a definir:\n",
    "Recompensa = {\n",
    "    0: (1 si gana, 0 si suma, -1 si no suma),\n",
    "    1:  (1 si suma, -1 si no suma),\n",
    "    2: (puntos si suma, -puntos si no suma),\n",
    "    3: (puntos/#tiros si suma, -puntos/#tiros si no suma),\n",
    "}\n",
    "\n",
    "Rango = RANGOS 1, RANGOS2, RANGOS3\n",
    "Alpha = [0,1]\n",
    "Gamma = [0,1]\n",
    "Epsilon = [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación DeepPurple: 25.066\n",
      "Validación Random: 26.927\n"
     ]
    }
   ],
   "source": [
    "from jugador import JugadorAleatorio\n",
    "from template import Validador\n",
    "\n",
    "\n",
    "val = Validador(ambiente)\n",
    "val_count = 1000\n",
    "avg = val.validar_politica(agente.q_table2pol(), val_count)\n",
    "print(f\"Validación DeepPurple: {avg}\")\n",
    "\n",
    "val_rand = Validador(ambiente)\n",
    "jugador = JugadorAleatorio(\"random\")\n",
    "avg_rand = val_rand.validar_jugador(jugador, val_count)\n",
    "print(f\"Validación Random: {avg_rand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "# import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "ambiente = AmbienteDiezMil()\n",
    "\n",
    "# Crear un agente de Q-learning\n",
    "agente = AgenteQLearning(ambiente)\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i, val in enumerate(agente.entrenar(episodios, verbose=True)):\n",
    "    vals.append(val)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(vals)\n",
    "    display.clear_output(wait=True)\n",
    "    # display.display(pl.gcf())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
